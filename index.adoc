= Blind Adversarial Attacks Against Medical Deep Learning Systems
:imagesdir: images
:icons: font
:date: May 16, 2019
:my_name: Peter Steinbach
:my_email: steinbach@extern.mpi-cbg.de
:my_twitter: psteinb_
:my_github: psteinb
:revealjs_slideNumber: true
:revealjs_center: true
:revealjs_BackgroundVertical: null
:revealjs_width: 1920
:revealjs_hash: true
:revealjs_margin: .05
:revealjs_plugin_pdf: enabled #you run your presentation in a browser with ?print-pdf at the end of the URL, you can then use the default print function to print the slide deck into a PDF document.
:customcss: custom.css
:source-highlighter: highlightjs
:stem:

mailto:{my_email}[{my_name}] (https://www.mpi-cbg.de[MPI CBG], https://www.scionics.de[Scionics]) +
2nd MLC Workshop {date} +
Helmholtz-Zentrum Dresden-Rossendorf, Germany +
https://twitter.com/{my_twitter}[icon:twitter[] psteinb_] https://github.com/{my_github}[icon:github[] psteinb] + 

== Introducing Adversarial Examples

icon:question-circle[5x]

=== Supervised Deep Learning Classifiers

image:dl-process.png[width=75%]

From <<MapRBlog>>

[.notes]
--
* classification (accuracy better than human)
* large training/test set
* data set = good balance of classes
--

=== What are Adversarial Examples?

image:advex-explained-1-nogradient.png[width=90%]

From <<Goodfellow14>> (based on <<Szegedy14>>)

=== Fooling by printing

image:advex-explained-2.png[width=90%]

From <<Kurakin16>>

=== Fooling by backdoors

image:badnet-without-caption.png[width=50%]

From <<Gu17>>


=== Consequences?

- security of autonomous driving
- security of voice recognition systems (Siri, Alexa, ...)
- product quality of online tools
- trust in automated decision tools
- trust in scientific classifiers

WARNING: [.XL-text]#Affects any deployed ML system!#



== Adversarial Attacks Against Medical Deep Learning Systems

=== Doctor's Appointment 

icon:user-md[5x]

=== Measurement

icon:heartbeat[5x]

=== Diagnosis

icon:medkit[5x] &nbsp; or &nbsp; icon:heart[5x]

=== Adversarial Attacks Against Medical Deep Learning Systems

[.XL-text]#Samuel G. Finlayson, Hyung Won Chung, Isaac S. Kohane, Andrew L. Beam (HMS+Harvard+MIT)# +

Science, 22 Mar 2019, Vol. 363, Issue 6433, pp. 1287-1289, https://doi.org/10.1126/science.aaw4399[10.1126/science.aaw4399]

[quote]
____
The prospect of improving healthcare and medicine with the use
of deep learning is truly exciting. [...]
it seems inevitable that medical deep learning algorithms will become entrenched in the already multi-billion dollar medical information technology industry.
____

[NOTE.speaker]
--
- published in Science
- reproduced it
--

[.XL-text]
=== Conclusion

[quote, Finlayson et al. "Adversarial Attacks Against Medical Deep Learning Systems"]
____
In this work, we have outlined the systemic and technological reasons that cause adversarial examples to pose a disproportion-
ately large threat in the medical domain, and provided examples of how such attacks may be executed.
____


**Can this be reproduced ?**


=== 3 open datasets

- https://www.kaggle.com/c/diabetic-retinopathy-detection/data[Diabetic Retinopathy Detection]
- http://academictorrents.com/details/557481faacd824c83fbf57dcf7b6da9383b3235a[X-ray Dataset of 14 Common Thorax Disease]
- https://www.isic-archive.com/#!/topWithHeader/onlyHeaderTop/gallery[skin cancer dataset] (omitted from this presentation)

=== https://www.kaggle.com/c/diabetic-retinopathy-detection/data[Diabetic Retinopathy] (DR)

[cols=">.^35,<.^65"]
|===
a|
image:dr-original.png[width=80%,align=right]

a|
* https://www.kaggle.com/c/diabetic-retinopathy-detection/data[kaggle dataset]: https://en.wikipedia.org/wiki/Ophthalmoscopy[fundus photographs]
* Diabetic retinopathy is the leading cause of blindness in the working-age population of the developed world.
* Clinicians can identify DR by the presence of lesions (vascular abnormalities due to disease)
* 88702 images, max. 4752x3168 RGB jpegs
* 5 labels (severity of DR)

|===



=== http://academictorrents.com/details/557481faacd824c83fbf57dcf7b6da9383b3235a[ChestX-ray8]

[cols=">.^35,<.^65"]
|===
a|
image:cxr-original.png[width=80%,align=right]

a|
* Classification and Localization of Common Thorax Diseases
* 112,120 images
* 1024x1024 8-bit greyscale
* 14 labels, appearing non-exclusively

|===

=== Data Access

* Diabetic Retinopathy Dataset
** 83 GB shared through kaggle

* X-ray Dataset
** 42 GB shared through science torrent 

IMPORTANT: Very Good!

[.notes]
--
* download of datasets was somewhat easy
* kaggle required to create an account
* omitted dataset used custom formatted REST API without front-end library :/
--

=== Code

https://github.com/sgfin/adversarial-medicine[github.com/sgfin/adversarial-medicine]

[%step]
- notebooks reproduce all figures based on downloaded weights
- python script to retrain two architectures (https://keras.io/applications/#resnet[Resnet50], https://keras.io/applications/#inceptionv3[InceptionV3])

=== Code Quality

[%step]
- code was executable once dropbox weights downloaded
- notebooks work from top to bottom
- code largely undocumented
- paper reports Resnet50 results, weights are for InceptionV3
- readme reports similar results for either


== My Rebuttal

=== Claim

[quote, Finlayson et al. "Adversarial Attacks Against Medical Deep Learning Systems"]
____
For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful.
____


=== White Box Attacks?
:stem: latexmath

Given:

[asciimath] 
++++ 
\vec{y} = f(\vec{x}, \theta), L( \vec{y}^{true}, f(\vec{x}, \theta))
++++

Weight Update Rule:

[asciimath] 
++++ 
\theta_{i+1} = \theta_{i} + \eta \nabla_{\theta}L( \vec{y}^{true}, f(\vec{x}, \theta_i))
++++


=== Projected Gradient Descent Attack


[asciimath] 
++++ 
x^{t+1} = Clip_{x,S} (x^{t} + \epsilon \sign(\nabla_x L(\theta,x,y)))
++++

[%step]
* iterate until stem:[x^{t+1}] is missclassified
* For a given set of allowed variations stem:[S]
* PGD <<Madry17>>,<<Kurakin1611>> is a White Box Attack!

[.notes]
--
* Using optimisation to counter optimisation
--


=== Taxonomy based on <<Biggio18>>

[cols="4*",options="header"]
|===
|type
|white box
|grey box
|black box

|adversary knows
|all of
|subset of
|only queries

a| &nbsp;
a| 
* dataset
* feature set
* learning alg + loss
* trained weights
a| 
* [.graytext]#dataset#
* feature set
* learning alg + loss
* [.graytext]#trained weights#
a| 
* [.graytext]#dataset#
* [.graytext]#feature set#
* [.graytext]#learning alg + loss#
* [.graytext]#trained weights#
|===

WARNING: [.XL-text]#Boundaries not clear cut!#

=== Black-Box Attacks assume knowledge

[quote]
____
[...] black box attacks were performed by crafting the attack against an independently-trained model with the same architecture and then transferring the resultant adversarial examples to the victim.
____

=== My Motivation

icon:question-circle[5x]

How about real black-box attacks?

=== https://github.com/bethgelab/foolbox[Foolbox]

[source,python,align="center"]
----
import foolbox
import keras
import numpy as np

keras_model = ...#load weights

fmodel = foolbox.models.KerasModel(keras_model, bounds=(0, 255),
				   preprocessing=preprocessing)

image, label = foolbox.utils.imagenet_example()

attack = foolbox.attacks.LinfinityBasicIterativeAttack(fmodel)
adversarial = attack(image[:, :, ::-1], label)
----

* offers a variety of white and black box attacks
* simple API can wrap models from TensorFlow, PyTorch, Theano, Keras, Lasagne, MXNet

=== Pure Black-box on Diabetic Retinopathy

[cols="^.^,^.^,^.^",width=100%,frame=none,grid=none]
|===
a|
[.text-center]
.Original
image:dr-original.png[width=80%]
a|
[.text-center]
.Adversarial
image:dr-adversarial.png[width=80%]
a|
[.text-center]
.Difference*20
image:dr-diff.png[width=80%]
|===

[.L-text,source,python]
----
attack = foolbox.attacks.SaltAndPepperNoiseAttack(fmodel)
----

[.notes]
--
* real world: shot noise in CCM chip
--

=== Easy! But why?

[cols="2*",width=100%,frame=none,grid=none]
|===
a|
[.text-center]
.Class Inbalance
image:dr_class_overview.svg[width=80%]

a|
[source,python]
----
train_generator = train_datagen.flow_from_directory(
            'images/train',
            target_size=(224, 224),
            batch_size=batch_size,
            class_mode='categorical',
            shuffle=True)
----

* Input image sizes range  [2500-4000]x[2000-2500]
* before training, resized to 224x224
* eye doctor: "DR pathologies are very fine structures!"

|===

[.notes]
--
* inbalanced class frequencies in training dataset
* real world: DR discovered in very fine structured elements
* subject to further study
--

=== DR Pathologies

[cols="2*^.^",frame=none,grid=none]
|===
a|
[.text-center]
.Healthy
image:dr-10-left-level0_800px.jpg[width=80%]
a|
[.text-center]
.Malignant
image:dr-16-left-level4_800px.jpg[width=80%]
|===


=== Chest X-Ray Dataset?


[cols="2*^.^",frame=none,grid=none,width=80%]
|===
a|
[.text-center]
&nbsp; +
&nbsp; +
No simple attack worked: +

* `SaltAndPepperNoiseAttack`
* `GaussianBlurAttack`
* `AdditiveUniformNoiseAttack`
* `ContrastReductionAttack`
a|
[.text-center]
&nbsp; +
&nbsp; +
image:cxr-original.png[width=50%,align=center]
|===

=== Decision Boundary Attack

image:1712.04248_fig2.png[width=60%,float=left]

&nbsp;
&nbsp;

Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models. +
Wieland Brendel, Jonas Rauber and Matthias Bethge +
arXiv preprint https://arxiv.org/abs/1712.04248[arXiv:1712.04248] (2017)


=== Pure Black-box Attacks on Chest X-Ray

[cols="^.<,^.<,^.<",width=100%,frame=none,grid=none]
|===
a| 
[.text-center]
.Original
image:cxr-original.png[width=80%]
`predict(label) = healthy`
a|
[.text-center]
.Adversarial Template
image:cxr-adversarial-template.png[width=80%]
`predict(label) = malignant`
a|
[.text-center]
.Adversarial
image:cxr-adversarial.png[width=80%]
`predict(label) = malignant`
|===

[.notes]
--
* model's predict function was called ~5000 times
--

== Summary

[role=L-text]
=== Adversarial Attacks Against Medical Deep Learning Systems

[%step]
* robustness of DL classifiers as diagnostic systems
* successfully achieved reproducibility (praise to the authors)
* illustrated classifiers and attacks appear remote to reality
* portions of the paper contradictory in the details

[role=XL-text]
=== Bigger Picture

[%step]
* end-to-end DL may not produce machine intelligence (data intelligence?)
* if data is all you got, better treat it carefully!
* adversarial examples = tool to probe resilience of your classifiers



include::references.adoc[]

